{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import random\n",
    "UBIT ='manishre'\n",
    "import numpy as np\n",
    "np.random.seed(sum([ord(c) for c in UBIT]))\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image1 = cv2.imread('mountain1.jpg')\n",
    "image2 = cv2.imread('mountain2.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(244, 1, 2)\n",
      "(244, 1, 2)\n"
     ]
    }
   ],
   "source": [
    "# SIFT OBJECT CREATION\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "# CALUCLATING the keypoints and descriptors\n",
    "detected_points1, descriptors1 = sift.detectAndCompute(image1,None)\n",
    "detected_points2, descriptors2 = sift.detectAndCompute(image2,None)\n",
    "# displaying the keypoints of the images\n",
    "detect_image1 =cv2.drawKeypoints(image1,detected_points1,None,flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "detect_image2 = cv2.drawKeypoints(image2,detected_points2,None,flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "#=====================================part2===========================================================================\n",
    "# Feature Matching \n",
    "Flann_index = 0\n",
    "index_parameters = dict(algorithm = Flann_index, trees = 5)\n",
    "search_parameters = dict(checks=50)   # or pass empty dictionary\n",
    "\n",
    "flann = cv2.FlannBasedMatcher(index_parameters,search_parameters)\n",
    "\n",
    "matches = flann.knnMatch(descriptors1,descriptors2,k=2)\n",
    "\n",
    "matchesMask = [[0,0] for i in range(len(matches))]\n",
    "#Apply ratio test\n",
    "good = []\n",
    "good_pts =[]\n",
    "for i,(m,n) in enumerate(matches):\n",
    "    if m.distance < 0.75*n.distance:\n",
    "        good.append([m])\n",
    "        good_pts.append(m)\n",
    "\n",
    "\n",
    "featureMacthing = cv2.drawMatchesKnn(image1,detected_points1,image2,detected_points2,good,None,flags=2)\n",
    "\n",
    "#=======================================part3==============================================================================\n",
    "src_pts = np.float32([ detected_points1[m.queryIdx].pt for m in good_pts ]).reshape(-1,1,2)\n",
    "dst_pts = np.float32([ detected_points2[m.trainIdx].pt for m in good_pts ]).reshape(-1,1,2)\n",
    "print(src_pts.shape)\n",
    "print(dst_pts.shape)\n",
    "homography, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC)\n",
    "#print(mask)\n",
    "#=====================================part4================================================================================\n",
    "inliner =[]\n",
    "for i in range(len(mask)):\n",
    "    if(mask[i]==1):\n",
    "        inliner.append(good[i])\n",
    "        #print(i)\n",
    "\n",
    "wrappi\n",
    "__10match = [] \n",
    "    \n",
    "__10match = random.sample(inliner,10)\n",
    "\n",
    "\n",
    "__10inlinerMatch = cv2.drawMatchesKnn(image1,detected_points1,image2,detected_points2,__10match,None,(255,0,0),flags=2)\n",
    "\n",
    "#====================================part5===================================================================================\n",
    "\n",
    "row1 = image1.shape[0]\n",
    "row2 = image2.shape[0]\n",
    "col1 = image1.shape[1]\n",
    "col2 = image2.shape[1]\n",
    "\n",
    "points_1 = np.float32([[0,0], [0,row1], [col1, row1], [col1,0]]).reshape(-1,1,2)\n",
    "temp = np.float32([[0,0], [0,row2], [col2, row2], [col2,0]]).reshape(-1,1,2)\n",
    "\n",
    "points_2 = cv2.perspectiveTransform(temp, homography)\n",
    "points = np.concatenate((points_1, points_2), axis=0)\n",
    "\n",
    "[x_min, y_min] = np.int32(points.min(axis=0).ravel() - 0.5)\n",
    "[x_max, y_max] = np.int32(points.max(axis=0).ravel() + 0.5)\n",
    "\n",
    "translation_dist = [-x_min, -y_min]\n",
    "H_translation = np.array([[1, 0, translation_dist[0]], [0, 1, translation_dist[1]], [0,0,1]])\n",
    "\n",
    "output_img = cv2.warpPerspective(image1, H_translation.dot(homography), (x_max - x_min, y_max - y_min))\n",
    "output_img[translation_dist[1]:row1+translation_dist[1],translation_dist[0]:col1+translation_dist[0]] = image2\n",
    "\n",
    "\n",
    "wrappedImage = cv2.warpPerspective(image1, homography, (image2.shape[1],image2.shape[0]))\n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======HOMOGRAPHY MATRIX=====\n",
      "[[ 1.58840616e+00 -2.91461287e-01 -3.95621049e+02]\n",
      " [ 4.45312759e-01  1.43782325e+00 -1.90624961e+02]\n",
      " [ 1.19636606e-03 -3.75347069e-05  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "cv2.imshow('keypoint1',detect_image1)\n",
    "cv2.waitKey(0)\n",
    "cv2.imshow('keypoints2',detect_image2)\n",
    "cv2.waitKey(0)\n",
    "cv2.imshow('Feature Matching',featureMacthing)\n",
    "cv2.waitKey(0)\n",
    "cv2.imshow('10 random inliner matching',__10inlinerMatch)\n",
    "cv2.waitKey(0)\n",
    "cv2.imshow('wrap',output_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "print(\"======HOMOGRAPHY MATRIX=====\")\n",
    "print(homography)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite('task1_sift1.jpg',detect_image1)\n",
    "cv2.imwrite('task1_sift2.jpg',detect_image2)\n",
    "cv2.imwrite('task1_matches_knn.jpg',featureMacthing)\n",
    "cv2.imwrite('task1_matches.jpg',__10inlinerMatch)\n",
    "cv2.imwrite('task1_pano.jpg',output_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "error1:\n",
    "https://www.pyimagesearch.com/2015/07/16/where-did-sift-and-surf-go-in-opencv-3/ for xfeatures2d also has the description for caluclating descriptors\n",
    "error2:\n",
    "draw keypoint takes 3 pos input ---> https://github.com/opencv/opencv/issues/6487 dont know why need to search it\n",
    "\n",
    "error3:\n",
    "TypeError: Expected cv::KeyPoint for argument 'keypoints1' \n",
    "idot you have passed the image instead of keypoints.\n",
    "\n",
    "error:4\n",
    "How to convert Keypoints to an argument for findhomography()?\n",
    "http://answers.opencv.org/question/122802/how-to-convert-keypoints-to-an-argument-for-findhomography/\n",
    "\n",
    "error5:\n",
    "wrap images\n",
    "https://www.kaggle.com/asymptote/homography-estimate-stitching-two-imag/code\n",
    "\n",
    "%===========================================================================================================================%\n",
    "Findings and references:\n",
    "1. caluclation of descriptors are done by using different feature matching techniques:\n",
    "2. like bruteforce,FLANN matchers,KNN match.\n",
    "\n",
    "link1  : https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_matcher/py_matcher.html\n",
    "\n",
    "link2  : https://docs.opencv.org/2.4/modules/features2d/doc/common_interfaces_of_descriptor_matchers.html\n",
    "\n",
    "link3  : https://docs.opencv.org/3.4/db/d39/classcv_1_1DescriptorMatcher.html --> bruteforce and FLANN matchers\n",
    "\n",
    "link4  : https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_feature2d/py_matcher/py_matcher.html --> knn matcher program\n",
    "\n",
    "link5  : https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_sift_intro/py_sift_intro.html\n",
    "-->sift detection\n",
    "\n",
    "link6  : https://docs.opencv.org/3.4.1/d9/dab/tutorial_homography.html --> homographic matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are descriptors?\n",
    "\n",
    "Now keypoint descriptor is created. A 16x16 neighbourhood around the keypoint is taken. It is devided into 16 sub-blocks of 4x4 size. For each sub-block, 8 bin orientation histogram is created. So a total of 128 bin values are available. It is represented as a vector to form keypoint descriptor. In addition to this, several measures are taken to achieve robustness against illumination changes, rotation etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
